{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from statistics import mode\n",
    "\n",
    "# lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def make_csvs(filenames, datasets, rows=False):\n",
    "    for j in range(4):\n",
    "        with open(filenames[j], \"w+\") as my_csv:\n",
    "            csvWriter = csv.writer(my_csv,delimiter=' ')\n",
    "            if rows:\n",
    "                csvWriter.writerows(datasets[j])\n",
    "            else: \n",
    "                csvWriter.writerow(datasets[j])\n",
    "\n",
    "def generate_files(window_size, overlap_size, training_split):\n",
    "    \n",
    "    X, Y, Z, actv = [], [], [], []\n",
    "\n",
    "    # pull data from file, split into lists\n",
    "    with open('./DIMDataset/clean_combined.csv', 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split(',')\n",
    "\n",
    "            X.append(int(line_list[0]))\n",
    "            Y.append(int(line_list[1]))\n",
    "            Z.append(int(line_list[2]))\n",
    "            actv.append(int(line_list[3]))\n",
    "        print('Samples:', len(X))\n",
    "\n",
    "    # write streams to file    \n",
    "    stream_filenames = ['./DIMDataset/stream_x.csv', './DIMDataset/stream_y.csv', \n",
    "                 './DIMDataset/stream_z.csv', './DIMDataset/stream_actv.csv']\n",
    "    datasets = [X, Y, Z, actv]\n",
    "    make_csvs(stream_filenames, datasets)\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    # group streams into windows\n",
    "    #window_size = 64 # 64 = 1sec windows\n",
    "    #overlap_size = 16\n",
    "\n",
    "    X_windows = [X[x:x + window_size] for x in range(0, len(X), window_size - overlap_size)]\n",
    "    Y_windows = [Y[x:x + window_size] for x in range(0, len(Y), window_size - overlap_size)]\n",
    "    Z_windows = [Z[x:x + window_size] for x in range(0, len(Z), window_size - overlap_size)]\n",
    "    actv_windows = [actv[x:x + window_size] for x in range(0, len(actv), window_size - overlap_size)]\n",
    "\n",
    "    # normalize last chunk with mean values\n",
    "    for item in [X_windows, Y_windows, Z_windows, actv_windows]:\n",
    "        if len(item[-1]) < window_size:\n",
    "            item[-1] = item[-1] + [int(mean(item[-1]))] * (window_size-len(item[-1]))\n",
    "    print('Windows:', len(X_windows))\n",
    "\n",
    "    for ii in range(len(actv_windows)):\n",
    "        try:\n",
    "            actv_windows[ii] = [mode(actv_windows[ii])]\n",
    "        except:\n",
    "            actv_windows[ii] = [int(round(mean(actv_windows[ii])))]\n",
    "\n",
    "    # write windows to file\n",
    "    window_filenames = ['./DIMDataset/windows_x.csv', './DIMDataset/windows_y.csv', \n",
    "                 './DIMDataset/windows_z.csv', './DIMDataset/windows_actv.csv']\n",
    "    datasets = [X_windows, Y_windows, Z_windows, actv_windows]\n",
    "    make_csvs(window_filenames, datasets, rows=True)\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    marker = int(len(X_windows) * training_split)\n",
    "    train_filenames = ['./DIMDataset/train/empatica/acc_x_train.csv', \n",
    "                       './DIMDataset/train/empatica/acc_y_train.csv',                     \n",
    "                       './DIMDataset/train/empatica/acc_z_train.csv', \n",
    "                       './DIMDataset/train/actv_train.csv']\n",
    "\n",
    "    test_filenames = ['./DIMDataset/test/empatica/acc_x_test.csv', \n",
    "                      './DIMDataset/test/empatica/acc_y_test.csv', \n",
    "                      './DIMDataset/test/empatica/acc_z_test.csv', \n",
    "                      './DIMDataset/test/actv_test.csv']\n",
    "\n",
    "    os.makedirs('./DIMDataset/train/empatica/', exist_ok=True)\n",
    "    os.makedirs('./DIMDataset/test/empatica/', exist_ok=True)\n",
    "\n",
    "    for i in range(len(datasets)):\n",
    "        train = datasets[i][:marker]\n",
    "        test = datasets[i][marker:]\n",
    "\n",
    "        with open(train_filenames[i], 'w+') as train_f:\n",
    "            csvWriter = csv.writer(train_f,delimiter=' ')\n",
    "            csvWriter.writerows(train)\n",
    "\n",
    "        with open(test_filenames[i], 'w+') as test_f:\n",
    "            csvWriter = csv.writer(test_f,delimiter=' ')\n",
    "            csvWriter.writerows(test)\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/empatica/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['acc_x_'+group+'.csv', 'acc_y_'+group+'.csv', 'acc_z_'+group+'.csv']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/actv_'+group+'.csv')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'DIMDataset/')\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'DIMDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    #print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    return m\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    return summarize_results(scores)\n",
    "\n",
    "def run_once(window_size, overlap_size, training_split):\n",
    "    # run the experiment\n",
    "    start_time = time.time()\n",
    "    generate_files(window_size, int(window_size*overlap_size), training_split)\n",
    "    res = run_experiment()\n",
    "    \n",
    "    print('\\n','Mean Accuracy:', res)\n",
    "    print(\"total time elapsed\", int((time.time() - start_time) / 60), \" min\")\n",
    "\n",
    "def run_sweep():\n",
    "    # sweep the parameters\n",
    "    windows = [16, 32, 48, 64, 80, 96, 112, 128]\n",
    "    overlap = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    training = [0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with open('results.csv', 'w+') as f:\n",
    "        for i in windows:\n",
    "            for j in overlap:\n",
    "                for k in training:\n",
    "                    print(' '.join([str(i), str(int(j*i)), str(k)]))\n",
    "                    \n",
    "                    generate_files(i, int(j*i), k)\n",
    "                    res = run_experiment()\n",
    "                    \n",
    "                    f.write(' '.join([str(i), str(j), str(k), str(res), '\\n']))\n",
    "                    print('acc:', res)\n",
    "                    print('time:', int((time.time() - start_time) / 60), \" min\")\n",
    "                    print()\n",
    "    print(\"total time elapsed\", int((time.time() - start_time) / 60), \" min\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 6441\n",
      "Windows: 806\n",
      "Saved model to disk\n",
      "\n",
      " Mean Accuracy: 94.73684212371661\n",
      "total time elapsed 0  min\n"
     ]
    }
   ],
   "source": [
    "# best settings 112, 0.2-0.3, 0.5\n",
    "run_once(16, 0.5, 0.6)\n",
    "# run_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01bea54d4c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# score = loaded_model.evaluate(X, Y, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testX' is not defined"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# predict with model here\n",
    "_, accuracy = loaded_model.evaluate(testX, testy, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
